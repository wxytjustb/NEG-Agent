from langgraph.graph import END  # type: ignore
from app.modules.workflow.core.graph import WorkflowGraphBuilder
from app.modules.workflow.core.state import WorkflowState
from app.modules.workflow.nodes.Intent_recognition import detect_intent
from app.modules.workflow.nodes.llm_answer import async_llm_stream_answer_node
from app.modules.workflow.nodes.user_info import async_user_info_node  # å¼‚æ­¥ç‰ˆæœ¬ï¼ˆæ”¯æŒ session ç¼“å­˜ï¼‰
from app.modules.workflow.nodes.chromadb_node import get_memory_node, save_memory_node  # ChromaDB è®°å¿†èŠ‚ç‚¹
from typing import Dict, Any, Optional
from lmnr import observe, Laminar
import logging

logger = logging.getLogger(__name__)


@observe(name="intent_recognition_node", tags=["node", "intent"])
def intent_recognition_node(state: WorkflowState) -> Dict[str, Any]:
    """æ„å›¾è¯†åˆ«èŠ‚ç‚¹ - LangGraph èŠ‚ç‚¹åŒ…è£…å™¨"""
    logger.info("========== æ„å›¾è¯†åˆ«èŠ‚ç‚¹å¼€å§‹ ===========")
    
    try:
        user_input = state.get("user_input", "")
        logger.info(f"ç”¨æˆ·è¾“å…¥: {user_input[:50]}...")
        
        # è°ƒç”¨æ„å›¾è¯†åˆ«
        intent, confidence, all_scores = detect_intent(user_input)
        
        logger.info(f"âœ… æ„å›¾è¯†åˆ«å®Œæˆ: {intent} (ç½®ä¿¡åº¦: {confidence:.2f})")
        
        # è¿”å›æ›´æ–°çš„çŠ¶æ€
        return {
            "intent": intent,
            "intent_confidence": confidence,
            "intent_scores": all_scores
        }
        
    except Exception as e:
        error_msg = f"æ„å›¾è¯†åˆ«èŠ‚ç‚¹æ‰§è¡Œå¤±è´¥: {str(e)}"
        logger.error(error_msg)
        return {
            "intent": "æ—¥å¸¸å¯¹è¯",
            "intent_confidence": 0.0,
            "intent_scores": {},
            "error": error_msg
        }


def create_chat_workflow():
    """åˆ›å»ºå¯¹è¯å·¥ä½œæµ"""
    logger.info("æ­£åœ¨åˆ›å»ºå¯¹è¯å·¥ä½œæµ...")
    
    # 1. åˆ›å»ºå›¾æ„å»ºå™¨
    builder = WorkflowGraphBuilder(state_schema=WorkflowState)
    
    # 2. æ·»åŠ èŠ‚ç‚¹ï¼ˆæŒ‰æ‰§è¡Œé¡ºåºï¼‰
    builder.add_node("user_info", async_user_info_node)           # ç¬¬1æ­¥ï¼šè·å–ç”¨æˆ·ç”»åƒ
    builder.add_node("intent_recognition", intent_recognition_node) # ç¬¬2æ­¥ï¼šæ„å›¾è¯†åˆ«
    builder.add_node("get_memory", get_memory_node)         # ç¬¬3æ­¥ï¼šè·å–å†å²è®°å¿†
    builder.add_node("llm_answer", async_llm_stream_answer_node)   # ç¬¬4æ­¥ï¼šLLMå›ç­”ï¼ˆå¼‚æ­¥æµå¼ï¼‰
    builder.add_node("save_memory", save_memory_node)       # ç¬¬5æ­¥ï¼šä¿å­˜è®°å¿†
    
    # 3. è®¾ç½®å…¥å£èŠ‚ç‚¹
    builder.set_entry_point("user_info")  # ä»ç”¨æˆ·ä¿¡æ¯è·å–å¼€å§‹
    
    # 4. æ·»åŠ è¾¹ï¼ˆè¿æ¥èŠ‚ç‚¹ï¼‰
    builder.add_edge("user_info", "intent_recognition")      # ç”¨æˆ·ä¿¡æ¯ â†’ æ„å›¾è¯†åˆ«
    builder.add_edge("intent_recognition", "get_memory")     # æ„å›¾è¯†åˆ« â†’ è·å–è®°å¿†
    builder.add_edge("get_memory", "llm_answer")             # è·å–è®°å¿† â†’ LLMå›ç­”
    builder.add_edge("llm_answer", "save_memory")            # LLMå›ç­” â†’ ä¿å­˜è®°å¿†
    builder.add_edge("save_memory", END)                     # ä¿å­˜è®°å¿† â†’ ç»“æŸ
    
    # 5. éªŒè¯å›¾ç»“æ„
    builder.validate()
    
    # 6. ç¼–è¯‘å›¾
    workflow = builder.compile()
    
    logger.info("âœ… å¯¹è¯å·¥ä½œæµåˆ›å»ºå®Œæˆ")
    logger.info("å·¥ä½œæµç»“æ„: ç”¨æˆ·ä¿¡æ¯ â†’ æ„å›¾è¯†åˆ« â†’ è·å–è®°å¿† â†’ LLMå›ç­” â†’ ä¿å­˜è®°å¿† â†’ ç»“æŸ")
    
    return workflow


# å…¨å±€å·¥ä½œæµå®ä¾‹ï¼ˆæ‡’åŠ è½½ï¼‰
_chat_workflow = None


def get_chat_workflow():
    """è·å–å¯¹è¯å·¥ä½œæµå®ä¾‹ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰
    
    Returns:
        ç¼–è¯‘åçš„å¯¹è¯å·¥ä½œæµ
    """
    global _chat_workflow
    
    if _chat_workflow is None:
        logger.info("é¦–æ¬¡è°ƒç”¨ï¼Œåˆ›å»ºå·¥ä½œæµå®ä¾‹...")
        _chat_workflow = create_chat_workflow()
    
    return _chat_workflow


@observe(name="chat_workflow_stream", tags=["workflow", "chat", "streaming"])
async def run_chat_workflow_streaming(
    user_input: str,
    session_id: str,
    user_id: Optional[str] = None,
    username: Optional[str] = None
):
    """è¿è¡Œå¯¹è¯å·¥ä½œæµï¼ˆæµå¼ç‰ˆæœ¬ï¼‰"""
    initial_state: WorkflowState = {
        "user_input": user_input,
        "session_id": session_id,
        "is_streaming": True
    }
    
    if user_id:
        initial_state["user_id"] = user_id
    
    # è®¾ç½® Laminar è¿½è¸ªå…ƒæ•°æ®
    if user_id:
        Laminar.set_trace_user_id(str(user_id))
    if session_id:
        Laminar.set_trace_session_id(session_id)
    
    Laminar.set_trace_metadata({
        "username": username or "Unknown",
        "user_id": str(user_id) if user_id else None,
        "session_id": session_id[:20] + "..." if len(session_id) > 20 else session_id,
        "message_preview": user_input[:50] + "..." if len(user_input) > 50 else user_input
    })
    
    config = {
        "metadata": {
            "workflow": "chat_workflow",
            "message": user_input[:50] + "..." if len(user_input) > 50 else user_input,
            "session_id": session_id,
            "user_id": str(user_id) if user_id else None,
            "username": username or "Unknown"
        }
    }
    
    has_output = False
    total_input_tokens = 0
    total_output_tokens = 0
    event_count = 0  # è°ƒè¯•ï¼šç»Ÿè®¡äº‹ä»¶æ•°é‡
    
    try:
        async for event in get_chat_workflow().astream_events(initial_state, config=config, version="v2"):
            event_type = event.get("event")
            event_count += 1
            
            # æ¯10ä¸ªäº‹ä»¶è®°å½•ä¸€æ¬¡ï¼ˆé¿å…æ—¥å¿—è¿‡å¤šï¼‰
            if event_count % 10 == 0:
                logger.debug(f"å·²å¤„ç† {event_count} ä¸ªäº‹ä»¶ï¼Œå½“å‰ç±»å‹: {event_type}")
            
            # ç›‘å¬ LLM Token ä½¿ç”¨æƒ…å†µ
            if event_type == "on_chat_model_end":
                output = event.get("data", {}).get("output")
                if output and hasattr(output, 'usage_metadata') and output.usage_metadata:
                    usage = output.usage_metadata
                    total_input_tokens += usage.get('input_tokens', 0)
                    total_output_tokens += usage.get('output_tokens', 0)
                    
                    # æ›´æ–° Laminar Span çš„ Token ç»Ÿè®¡
                    Laminar.set_span_attributes({
                        "llm.usage.input_tokens": total_input_tokens,
                        "llm.usage.output_tokens": total_output_tokens,
                        "llm.usage.total_tokens": total_input_tokens + total_output_tokens
                    })
            
            # å°è¯•ç›‘å¬å¤šç§æµå¼äº‹ä»¶ç±»å‹
            if event_type in ["on_chat_model_stream", "on_llm_stream", "on_chain_stream"]:
                chunk = event.get("data", {}).get("chunk")
                if chunk and hasattr(chunk, "content"):
                    content = chunk.content
                    if content:
                        has_output = True
                        logger.debug(f"æ•è·åˆ°æµå¼chunk ({event_type}): {content[:50]}...")
                        print(f"ğŸ”¥ æµå¼è¾“å‡º: [{content}]")  # æ·»åŠ è¿™è¡Œï¼Œè§‚å¯Ÿæ¯ä¸ª token
                        yield content
        
        logger.info(f"âœ… å·¥ä½œæµå®Œæˆ: äº‹ä»¶æ•°={event_count}, æµå¼è¾“å‡º={has_output}")
        
        # å…œåº•é€»è¾‘ï¼šä»…åœ¨å®Œå…¨æ²¡æœ‰è¾“å‡ºæ—¶è§¦å‘
        if not has_output:
            logger.warning("âš ï¸ æœªæ•è·åˆ°æµå¼è¾“å‡ºï¼Œä½¿ç”¨å…œåº•é€»è¾‘ï¼ˆä¸ä¼šé‡æ–°æ‰§è¡Œå·¥ä½œæµï¼‰")
            
            # âŒ ä¸è¦é‡æ–°æ‰§è¡Œå·¥ä½œæµï¼åªä»å·²å®Œæˆçš„çŠ¶æ€ä¸­è·å–ç»“æœ
            # è¿™é‡Œçš„é—®é¢˜æ˜¯ï¼šastream_events å·²ç»æ‰§è¡Œå®Œäº†å·¥ä½œæµï¼Œåªæ˜¯æ²¡æœ‰ yield å‡ºæ¥
            # æˆ‘ä»¬åº”è¯¥ä»æœ€ç»ˆçŠ¶æ€è·å–ç»“æœï¼Œè€Œä¸æ˜¯å†æ¬¡ invoke
            
            # ç”±äº astream_events ä¸è¿”å›æœ€ç»ˆçŠ¶æ€ï¼Œæˆ‘ä»¬åªèƒ½æç¤ºé”™è¯¯
            yield "[æç¤º] æµå¼è¾“å‡ºå¼‚å¸¸ï¼Œè¯·é‡è¯•"
    
    except Exception as e:
        logger.error(f"æµå¼å·¥ä½œæµæ‰§è¡Œå¤±è´¥: {str(e)}", exc_info=True)
        yield f"[é”™è¯¯] {str(e)}"
