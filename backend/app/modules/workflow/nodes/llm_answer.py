# LLM å›ç­”èŠ‚ç‚¹ - æ„å»ºå®Œæ•´ Prompt å¹¶è°ƒç”¨ LLM ç”Ÿæˆå›ç­”
from typing import Dict, Any
from app.modules.workflow.core.state import WorkflowState
from app.modules.llm.core.llm_core import llm_core
from app.utils.prompt import build_full_prompt, ANRAN_SYSTEM_PROMPT
from lmnr import observe
import logging

logger = logging.getLogger(__name__)


@observe(name="llm_answer_node", tags=["node", "llm", "generation"])
async def async_llm_stream_answer_node(state: WorkflowState):
    """LLM å¼‚æ­¥æµå¼å›ç­”èŠ‚ç‚¹ - ä¾› astream_events ä½¿ç”¨"""
    try:
        user_input = state.get("user_input", "")
        intent = state.get("intent", "æ—¥å¸¸å¯¹è¯")
        company = state.get("company", "æœªçŸ¥")
        age = state.get("age", "æœªçŸ¥")
        gender = state.get("gender", "æœªçŸ¥")
        history_text = state.get("history_text", "")
        
        full_prompt = build_full_prompt(
            user_input=user_input,
            history_text=history_text,
            company=company,
            age=age,
            gender=gender,
            current_intent=intent
        )
        
        llm = llm_core.create_llm(
            temperature=0.7,
            max_tokens=2000
        )
        
        # âœ… ä½¿ç”¨ astream äº§ç”Ÿæµå¼è¾“å‡ºï¼Œè®© astream_events èƒ½å¤Ÿç›‘å¬åˆ°æµå¼äº‹ä»¶
        # LangGraph çš„ astream_events ä¼šæ•è· on_chat_model_stream äº‹ä»¶
        logger.info("ğŸ”¥ å¼€å§‹è°ƒç”¨ LLM æµå¼ç”Ÿæˆ...")
        full_response = ""
        chunk_count = 0
        async for chunk in llm.astream(full_prompt):
            if hasattr(chunk, 'content') and chunk.content:
                chunk_count += 1
                full_response += chunk.content
                if chunk_count <= 5:  # åªè®°å½•å‰5ä¸ªchunk
                    logger.info(f"ğŸ”¥ LLM Chunk #{chunk_count}: [{chunk.content}]")
        
        logger.info(f"âœ… LLM æµå¼ç”Ÿæˆå®Œæˆï¼Œå…± {chunk_count} ä¸ª chunks")
        
        return {
            "full_prompt": full_prompt,
            "llm_response": full_response
        }
        
    except Exception as e:
        logger.error(f"LLM èŠ‚ç‚¹æ‰§è¡Œå¤±è´¥: {str(e)}", exc_info=True)
        return {
            "error": str(e),
            "full_prompt": "",
            "llm_response": "æŠ±æ­‰ï¼Œæˆ‘ç°åœ¨é‡åˆ°äº†ä¸€äº›æŠ€æœ¯é—®é¢˜ï¼Œè¯·ç¨åå†è¯•ã€‚"
        }
