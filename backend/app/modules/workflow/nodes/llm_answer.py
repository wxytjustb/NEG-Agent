# LLM å›ç­”èŠ‚ç‚¹ - æ„å»ºå®Œæ•´ Prompt å¹¶è°ƒç”¨ LLM ç”Ÿæˆå›ç­”
from typing import Dict, Any, Optional
from langchain_core.runnables import RunnableConfig
from app.modules.workflow.core.state import WorkflowState
from app.modules.llm.core.llm_core import llm_core
from app.utils.prompt import build_full_prompt, ANRAN_SYSTEM_PROMPT
from lmnr import observe
import logging

logger = logging.getLogger(__name__)


@observe(name="llm_answer_node", tags=["node", "llm", "generation"])
async def async_llm_stream_answer_node(state: WorkflowState, config: Optional[RunnableConfig] = None):
    """LLM å¼‚æ­¥æµå¼å›ç­”èŠ‚ç‚¹ - ä¾› astream_events ä½¿ç”¨"""
    try:
        user_input = state.get("user_input", "")
        intent = state.get("intent", "æ—¥å¸¸å¯¹è¯")
        intents = state.get("intents", [])  # æ–°å¢ï¼šè·å–æ‰€æœ‰æ„å›¾
        company = state.get("company", "æœªçŸ¥")
        age = state.get("age", "æœªçŸ¥")
        gender = state.get("gender", "æœªçŸ¥")
        history_text = state.get("history_text", "")  # ChromaDB å†å²æ¶ˆæ¯
        working_memory_text = state.get("working_memory_text", "")  # Redis çŸ­æœŸè®°å¿†
        similar_messages = state.get("similar_messages", "")  # ç›¸ä¼¼åº¦è¾ƒé«˜çš„æ¶ˆæ¯
        feedback_summary = state.get("feedback_summary", "")  # ç”¨æˆ·åé¦ˆè¶‹åŠ¿æ‘˜è¦
        
        full_prompt = build_full_prompt(
            user_input=user_input,
            working_memory_text=working_memory_text,  # ä¼ å…¥ working_memory_text
            history_text=history_text,
            similar_messages=similar_messages,
            company=company,
            age=age,
            gender=gender,
            current_intent=intent,
            intents=intents,  # æ–°å¢ï¼šä¼ å…¥æ‰€æœ‰æ„å›¾
            feedback_summary=feedback_summary  # æ–°å¢ï¼šä¼ å…¥åé¦ˆæ‘˜è¦
        )
        
        llm = llm_core.create_llm(
            temperature=0.7,
            max_tokens=2000
        )
        
        # ğŸ”¥ å…³é”®ï¼šä½¿ç”¨ ainvoke + configï¼Œè®© astream_events èƒ½æ•è·æµå¼äº‹ä»¶
        # å½“ streaming=True æ—¶ï¼Œainvoke å†…éƒ¨ä¼šæµå¼å¤„ç†ï¼Œastream_events èƒ½ç›‘å¬åˆ°
        # æ³¨å…¥ç‰¹æ®Š tag ä»¥ä¾¿åœ¨ workflow ä¸­è¿‡æ»¤
        if config:
            # ç¡®ä¿ä¸ä¿®æ”¹åŸå§‹ config å¯¹è±¡
            import copy
            config = copy.copy(config)
            tags = config.get("tags", [])
            if tags is None:
                tags = []
            if "answer_generator" not in tags:
                tags.append("answer_generator")
            config["tags"] = tags
        else:
            config = {"tags": ["answer_generator"]}

        response = await llm.ainvoke(full_prompt, config=config)
        full_response = response.content if hasattr(response, 'content') else str(response)
        
        
        return {
            "full_prompt": full_prompt,
            "llm_response": full_response
        }
        
    except Exception as e:
        logger.error(f"LLM èŠ‚ç‚¹æ‰§è¡Œå¤±è´¥: {str(e)}", exc_info=True)
        return {
            "error": str(e),
            "full_prompt": "",
            "llm_response": "æŠ±æ­‰ï¼Œæˆ‘ç°åœ¨é‡åˆ°äº†ä¸€äº›æŠ€æœ¯é—®é¢˜ï¼Œè¯·ç¨åå†è¯•ã€‚"
        }